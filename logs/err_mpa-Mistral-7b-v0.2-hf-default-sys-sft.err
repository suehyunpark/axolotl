srun: error: NodeNames=localhost CPUs=# or Procs=# with Boards=# is invalid and is ignored.
/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3625 examples [00:00, 11783.40 examples/s]Generating train split: 7256 examples [00:00, 7761.27 examples/s] Generating train split: 10926 examples [00:01, 6118.07 examples/s]Generating train split: 14539 examples [00:04, 2340.91 examples/s]Generating train split: 18141 examples [00:05, 2693.95 examples/s]Generating train split: 21773 examples [00:06, 3294.43 examples/s]Generating train split: 25397 examples [00:06, 4039.06 examples/s]Generating train split: 29012 examples [00:06, 5271.76 examples/s]Generating train split: 32641 examples [00:07, 5936.47 examples/s]Generating train split: 36219 examples [00:08, 5686.82 examples/s]Generating train split: 39831 examples [00:09, 4048.85 examples/s]Generating train split: 43456 examples [00:19, 1025.45 examples/s]Generating train split: 47087 examples [00:20, 1294.15 examples/s]Generating train split: 50683 examples [00:20, 1729.29 examples/s]Generating train split: 54299 examples [00:27, 1025.76 examples/s]Generating train split: 57930 examples [00:28, 1344.98 examples/s]Generating train split: 61547 examples [00:29, 1691.02 examples/s]Generating train split: 63416 examples [00:29, 1920.78 examples/s]Generating train split: 63416 examples [00:29, 2120.61 examples/s]
Tokenizing Prompts (num_proc=64):   0%|          | 0/63416 [00:00<?, ? examples/s]Tokenizing Prompts (num_proc=64):   0%|          | 1/63416 [00:01<34:12:27,  1.94s/ examples]Tokenizing Prompts (num_proc=64):   1%|          | 780/63416 [00:02<01:57, 533.52 examples/s]Tokenizing Prompts (num_proc=64):   5%|▍         | 2875/63416 [00:02<00:25, 2341.18 examples/s]Tokenizing Prompts (num_proc=64):   8%|▊         | 5043/63416 [00:02<00:12, 4491.87 examples/s]Tokenizing Prompts (num_proc=64):  11%|█▏        | 7183/63416 [00:02<00:08, 6776.73 examples/s]Tokenizing Prompts (num_proc=64):  15%|█▍        | 9298/63416 [00:02<00:05, 9044.58 examples/s]Tokenizing Prompts (num_proc=64):  18%|█▊        | 11437/63416 [00:02<00:04, 11313.33 examples/s]Tokenizing Prompts (num_proc=64):  21%|██▏       | 13590/63416 [00:02<00:03, 13370.25 examples/s]Tokenizing Prompts (num_proc=64):  25%|██▍       | 15715/63416 [00:02<00:03, 15111.83 examples/s]Tokenizing Prompts (num_proc=64):  28%|██▊       | 17838/63416 [00:02<00:02, 16538.91 examples/s]Tokenizing Prompts (num_proc=64):  31%|███▏      | 19865/63416 [00:02<00:02, 17204.17 examples/s]Tokenizing Prompts (num_proc=64):  34%|███▍      | 21852/63416 [00:03<00:02, 17847.52 examples/s]Tokenizing Prompts (num_proc=64):  38%|███▊      | 23949/63416 [00:03<00:02, 18543.04 examples/s]Tokenizing Prompts (num_proc=64):  41%|████      | 26064/63416 [00:03<00:01, 19137.28 examples/s]Tokenizing Prompts (num_proc=64):  44%|████▍     | 28087/63416 [00:03<00:01, 19407.69 examples/s]Tokenizing Prompts (num_proc=64):  47%|████▋     | 30109/63416 [00:03<00:01, 18713.62 examples/s]Tokenizing Prompts (num_proc=64):  51%|█████     | 32111/63416 [00:03<00:01, 19072.60 examples/s]Tokenizing Prompts (num_proc=64):  54%|█████▍    | 34170/63416 [00:03<00:01, 19259.22 examples/s]Tokenizing Prompts (num_proc=64):  57%|█████▋    | 36283/63416 [00:03<00:01, 19747.31 examples/s]Tokenizing Prompts (num_proc=64):  61%|██████    | 38443/63416 [00:03<00:01, 19954.57 examples/s]Tokenizing Prompts (num_proc=64):  64%|██████▍   | 40683/63416 [00:04<00:01, 20422.01 examples/s]Tokenizing Prompts (num_proc=64):  68%|██████▊   | 42869/63416 [00:04<00:00, 20676.82 examples/s]Tokenizing Prompts (num_proc=64):  71%|███████   | 44961/63416 [00:04<00:00, 20532.82 examples/s]Tokenizing Prompts (num_proc=64):  74%|███████▍  | 47029/63416 [00:04<00:00, 20553.69 examples/s]Tokenizing Prompts (num_proc=64):  77%|███████▋  | 49089/63416 [00:04<00:00, 20564.75 examples/s]Tokenizing Prompts (num_proc=64):  81%|████████  | 51150/63416 [00:04<00:00, 20401.90 examples/s]Tokenizing Prompts (num_proc=64):  84%|████████▍ | 53202/63416 [00:04<00:00, 20164.61 examples/s]Tokenizing Prompts (num_proc=64):  87%|████████▋ | 55231/63416 [00:04<00:00, 20002.39 examples/s]Tokenizing Prompts (num_proc=64):  90%|█████████ | 57234/63416 [00:04<00:00, 19746.93 examples/s]Tokenizing Prompts (num_proc=64):  93%|█████████▎| 59213/63416 [00:04<00:00, 18857.60 examples/s]Tokenizing Prompts (num_proc=64):  96%|█████████▋| 61114/63416 [00:05<00:00, 15008.06 examples/s]Tokenizing Prompts (num_proc=64):  99%|█████████▉| 62742/63416 [00:05<00:00, 9187.64 examples/s] Tokenizing Prompts (num_proc=64): 100%|██████████| 63416/63416 [00:09<00:00, 6578.36 examples/s]
Dropping Long Sequences (num_proc=255):   0%|          | 0/63416 [00:00<?, ? examples/s]Dropping Long Sequences (num_proc=255):   0%|          | 249/63416 [00:00<01:24, 747.92 examples/s]Dropping Long Sequences (num_proc=255):   1%|          | 498/63416 [00:00<01:09, 901.66 examples/s]Dropping Long Sequences (num_proc=255):   2%|▏         | 996/63416 [00:00<00:39, 1566.16 examples/s]Dropping Long Sequences (num_proc=255):   9%|▉         | 5976/63416 [00:00<00:05, 10737.02 examples/s]Dropping Long Sequences (num_proc=255):  16%|█▌        | 9958/63416 [00:01<00:04, 12603.54 examples/s]Dropping Long Sequences (num_proc=255):  18%|█▊        | 11451/63416 [00:01<00:05, 9525.12 examples/s]Dropping Long Sequences (num_proc=255):  53%|█████▎    | 33608/63416 [00:01<00:00, 41714.74 examples/s]Dropping Long Sequences (num_proc=255):  63%|██████▎   | 39833/63416 [00:01<00:00, 41687.47 examples/s]Dropping Long Sequences (num_proc=255): 100%|██████████| 63416/63416 [00:01<00:00, 76825.74 examples/s]Dropping Long Sequences (num_proc=255): 100%|██████████| 63416/63416 [00:02<00:00, 30724.53 examples/s]
Add position_id column (Sample Packing) (num_proc=255):   0%|          | 0/63416 [00:00<?, ? examples/s]Add position_id column (Sample Packing) (num_proc=255):   0%|          | 44/63416 [00:00<03:50, 275.15 examples/s]Add position_id column (Sample Packing) (num_proc=255):   2%|▏         | 961/63416 [00:00<00:14, 4251.25 examples/s]Add position_id column (Sample Packing) (num_proc=255):   5%|▌         | 3312/63416 [00:00<00:05, 11591.60 examples/s]Add position_id column (Sample Packing) (num_proc=255):   8%|▊         | 5177/63416 [00:00<00:04, 14079.88 examples/s]Add position_id column (Sample Packing) (num_proc=255):  11%|█         | 6864/63416 [00:00<00:04, 13352.67 examples/s]Add position_id column (Sample Packing) (num_proc=255):  13%|█▎        | 8296/63416 [00:00<00:05, 10084.32 examples/s]Add position_id column (Sample Packing) (num_proc=255):  15%|█▍        | 9459/63416 [00:02<00:23, 2279.43 examples/s] Add position_id column (Sample Packing) (num_proc=255):  31%|███       | 19505/63416 [00:02<00:04, 8976.16 examples/s]Add position_id column (Sample Packing) (num_proc=255):  36%|███▌      | 22815/63416 [00:03<00:06, 6083.66 examples/s]Add position_id column (Sample Packing) (num_proc=255):  56%|█████▋    | 35738/63416 [00:03<00:01, 14089.27 examples/s]Add position_id column (Sample Packing) (num_proc=255):  93%|█████████▎| 59065/63416 [00:03<00:00, 32726.12 examples/s]Add position_id column (Sample Packing) (num_proc=255): 100%|██████████| 63416/63416 [00:04<00:00, 15658.19 examples/s]
Saving the dataset (0/2 shards):   0%|          | 0/63416 [00:00<?, ? examples/s]Saving the dataset (0/2 shards):   2%|▏         | 1000/63416 [00:00<00:06, 9328.20 examples/s]Saving the dataset (0/2 shards):   5%|▍         | 3000/63416 [00:00<00:05, 11761.28 examples/s]Saving the dataset (0/2 shards):   8%|▊         | 5000/63416 [00:00<00:04, 12184.86 examples/s]Saving the dataset (0/2 shards):  11%|█         | 7000/63416 [00:00<00:04, 12481.21 examples/s]Saving the dataset (0/2 shards):  14%|█▍        | 9000/63416 [00:00<00:04, 12477.31 examples/s]Saving the dataset (0/2 shards):  17%|█▋        | 11000/63416 [00:00<00:04, 12898.61 examples/s]Saving the dataset (0/2 shards):  20%|██        | 13000/63416 [00:01<00:03, 12782.27 examples/s]Saving the dataset (0/2 shards):  24%|██▎       | 15000/63416 [00:01<00:03, 12914.27 examples/s]Saving the dataset (0/2 shards):  27%|██▋       | 17000/63416 [00:01<00:03, 13370.46 examples/s]Saving the dataset (0/2 shards):  30%|██▉       | 19000/63416 [00:01<00:03, 13759.91 examples/s]Saving the dataset (0/2 shards):  33%|███▎      | 21000/63416 [00:01<00:03, 13837.30 examples/s]Saving the dataset (0/2 shards):  36%|███▋      | 23000/63416 [00:01<00:02, 13837.52 examples/s]Saving the dataset (0/2 shards):  39%|███▉      | 25000/63416 [00:01<00:02, 13829.05 examples/s]Saving the dataset (0/2 shards):  43%|████▎     | 27000/63416 [00:02<00:02, 13489.19 examples/s]Saving the dataset (0/2 shards):  46%|████▌     | 29000/63416 [00:02<00:02, 13697.57 examples/s]Saving the dataset (0/2 shards):  49%|████▉     | 31000/63416 [00:02<00:02, 13431.05 examples/s]Saving the dataset (1/2 shards):  50%|█████     | 31708/63416 [00:08<00:02, 13431.05 examples/s]Saving the dataset (1/2 shards):  50%|█████     | 31708/63416 [00:20<00:02, 13431.05 examples/s]Saving the dataset (1/2 shards):  52%|█████▏    | 32708/63416 [00:22<01:41, 303.93 examples/s]  Saving the dataset (1/2 shards):  53%|█████▎    | 33708/63416 [00:23<01:24, 350.75 examples/s]Saving the dataset (1/2 shards):  55%|█████▍    | 34708/63416 [00:24<01:10, 406.38 examples/s]Saving the dataset (1/2 shards):  56%|█████▋    | 35708/63416 [00:25<00:55, 495.26 examples/s]Saving the dataset (1/2 shards):  58%|█████▊    | 36708/63416 [00:25<00:41, 645.85 examples/s]Saving the dataset (1/2 shards):  61%|██████    | 38708/63416 [00:26<00:26, 941.13 examples/s]Saving the dataset (1/2 shards):  63%|██████▎   | 39708/63416 [00:26<00:20, 1183.97 examples/s]Saving the dataset (1/2 shards):  64%|██████▍   | 40708/63416 [00:30<00:37, 598.40 examples/s] Saving the dataset (1/2 shards):  64%|██████▍   | 40708/63416 [00:50<00:37, 598.40 examples/s]Saving the dataset (1/2 shards):  66%|██████▌   | 41708/63416 [01:17<04:49, 75.08 examples/s] Saving the dataset (1/2 shards):  67%|██████▋   | 42708/63416 [01:18<03:31, 98.07 examples/s]Saving the dataset (1/2 shards):  67%|██████▋   | 42708/63416 [01:30<03:31, 98.07 examples/s]Saving the dataset (1/2 shards):  69%|██████▉   | 43708/63416 [02:14<07:31, 43.67 examples/s]Saving the dataset (1/2 shards):  69%|██████▉   | 43708/63416 [02:30<07:31, 43.67 examples/s]Saving the dataset (1/2 shards):  70%|███████   | 44708/63416 [02:41<07:30, 41.57 examples/s]Saving the dataset (1/2 shards):  72%|███████▏  | 45708/63416 [02:46<05:26, 54.27 examples/s]Saving the dataset (1/2 shards):  74%|███████▎  | 46708/63416 [02:46<03:39, 75.99 examples/s]Saving the dataset (1/2 shards):  75%|███████▌  | 47708/63416 [02:46<02:26, 107.14 examples/s]Saving the dataset (1/2 shards):  75%|███████▌  | 47708/63416 [03:00<02:26, 107.14 examples/s]Saving the dataset (1/2 shards):  77%|███████▋  | 48708/63416 [03:10<03:19, 73.78 examples/s] Saving the dataset (1/2 shards):  77%|███████▋  | 48708/63416 [03:20<03:19, 73.78 examples/s]Saving the dataset (1/2 shards):  78%|███████▊  | 49708/63416 [03:38<04:06, 55.59 examples/s]Saving the dataset (1/2 shards):  78%|███████▊  | 49708/63416 [03:50<04:06, 55.59 examples/s]Saving the dataset (1/2 shards):  80%|███████▉  | 50708/63416 [04:10<04:40, 45.30 examples/s]Saving the dataset (1/2 shards):  80%|███████▉  | 50708/63416 [04:30<04:40, 45.30 examples/s]Saving the dataset (1/2 shards):  82%|████████▏ | 51708/63416 [04:42<04:54, 39.77 examples/s]Saving the dataset (1/2 shards):  83%|████████▎ | 52708/63416 [04:43<03:12, 55.74 examples/s]Saving the dataset (1/2 shards):  85%|████████▍ | 53708/63416 [04:53<02:31, 64.26 examples/s]Saving the dataset (1/2 shards):  85%|████████▍ | 53708/63416 [05:10<02:31, 64.26 examples/s]Saving the dataset (1/2 shards):  86%|████████▋ | 54708/63416 [05:23<02:51, 50.65 examples/s]Saving the dataset (1/2 shards):  88%|████████▊ | 55708/63416 [05:28<01:58, 64.86 examples/s]Saving the dataset (1/2 shards):  88%|████████▊ | 55708/63416 [05:40<01:58, 64.86 examples/s]Saving the dataset (1/2 shards):  89%|████████▉ | 56708/63416 [05:55<02:07, 52.75 examples/s]Saving the dataset (1/2 shards):  91%|█████████ | 57708/63416 [06:02<01:26, 65.71 examples/s]Saving the dataset (1/2 shards):  93%|█████████▎| 58708/63416 [06:17<01:11, 66.11 examples/s]Saving the dataset (1/2 shards):  94%|█████████▍| 59708/63416 [06:22<00:45, 81.96 examples/s]Saving the dataset (1/2 shards):  96%|█████████▌| 60708/63416 [06:35<00:33, 81.12 examples/s]Saving the dataset (1/2 shards):  96%|█████████▌| 60708/63416 [06:50<00:33, 81.12 examples/s]Saving the dataset (1/2 shards):  97%|█████████▋| 61708/63416 [06:56<00:25, 66.04 examples/s]Saving the dataset (1/2 shards):  99%|█████████▉| 62708/63416 [07:09<00:10, 69.66 examples/s]Saving the dataset (1/2 shards): 100%|██████████| 63416/63416 [07:15<00:00, 77.02 examples/s]Saving the dataset (2/2 shards): 100%|██████████| 63416/63416 [07:15<00:00, 77.02 examples/s]Saving the dataset (2/2 shards): 100%|██████████| 63416/63416 [07:15<00:00, 145.61 examples/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:00,  2.37it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:00,  2.31it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s]Downloading shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.32it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Downloading shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s]Downloading shards:  67%|██████▋   | 2/3 [00:01<00:00,  2.00it/s]Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]
Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.87it/s]Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]
Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]
Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]Downloading shards: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.91s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.70s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.62s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.53s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 184.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 135.32s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 183.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 135.28s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 183.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 135.32s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 183.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [06:45<00:00, 135.25s/it]
Traceback (most recent call last):
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mnt/nas/suehyun/axolotl/src/axolotl/cli/train.py", line 59, in <module>
    fire.Fire(do_cli)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/mnt/nas/suehyun/axolotl/src/axolotl/cli/train.py", line 35, in do_cli
    return do_train(parsed_cfg, parsed_cli_args)
  File "/mnt/nas/suehyun/axolotl/src/axolotl/cli/train.py", line 55, in do_train
    return train(cfg=cfg, cli_args=cli_args, dataset_meta=dataset_meta)
  File "/mnt/nas/suehyun/axolotl/src/axolotl/train.py", line 107, in train
    trainer = setup_trainer(
  File "/mnt/nas/suehyun/axolotl/src/axolotl/utils/trainer.py", line 353, in setup_trainer
    return trainer_builder.build(total_num_steps)
  File "/mnt/nas/suehyun/axolotl/src/axolotl/core/trainer_builder.py", line 1335, in build
    trainer = trainer_cls(
  File "/mnt/nas/suehyun/axolotl/src/axolotl/core/trainer_builder.py", line 235, in __init__
    super().__init__(*_args, **kwargs)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/trainer.py", line 554, in __init__
    self.init_hf_repo()
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/transformers/trainer.py", line 3910, in init_hf_repo
    repo_url = create_repo(repo_name, token=self.args.hub_token, private=self.args.hub_private_repo, exist_ok=True)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 111, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 159, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'kaist-ai/kaist-ai/mpa-Mistral-7b-v0.2-hf-default-sys-sft-66k'. Use `repo_type` argument if needed.
[2024-04-18 21:51:34,027] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 146508 closing signal SIGTERM
[2024-04-18 21:51:34,027] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 146509 closing signal SIGTERM
[2024-04-18 21:51:34,028] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 146510 closing signal SIGTERM
[2024-04-18 21:51:38,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 146507) of binary: /home/suehyun/.conda/envs/mpa/bin/python
Traceback (most recent call last):
  File "/home/suehyun/.conda/envs/mpa/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1048, in launch_command
    multi_gpu_launcher(args)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/accelerate/commands/launch.py", line 702, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/suehyun/.conda/envs/mpa/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
axolotl.cli.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-18_21:51:34
  host      : aigpu01
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 146507)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: localhost: task 0: Exited with exit code 1
